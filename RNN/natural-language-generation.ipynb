{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1214055characters\n",
      "\n",
      "['1\\r', '마\\r', 'ㅎㅇ\\r', '뭐야\\r', '늦었네 \\r']\n"
     ]
    }
   ],
   "source": [
    "train_text = open('input.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print(\"Length of text: {}characters\".format(len(train_text)))\n",
    "print()\n",
    "\n",
    "train_text = train_text.split('\\n')\n",
    "train_text = [sentence[17:] for sentence in train_text]\n",
    "\n",
    "print(train_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '마', '뭐야', '늦었네', '후', '버그거렸노', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '기타창에', '넣을', '아이콘', '뭐']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):    \n",
    "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    \n",
    "    return string\n",
    "\n",
    "\n",
    "train_text = [clean_str(sentence) for sentence in train_text]\n",
    "train_text_X = []\n",
    "for sentence in train_text:\n",
    "    train_text_X.extend(sentence.split(' '))\n",
    "    \n",
    "train_text_X = [word for word in train_text_X if word != '']\n",
    "\n",
    "print(train_text_X[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32554 unique words\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(train_text_X))\n",
    "vocab.append('UNK')\n",
    "print('{} unique words'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '!' :   0,\n",
      "  ',' :   1,\n",
      "  '0' :   2,\n",
      "  '00':   3,\n",
      "  '0000':   4,\n",
      "  '000000195259':   5,\n",
      "  '0000115286':   6,\n",
      "  '0000802645':   7,\n",
      "  '0000프로':   8,\n",
      "  '0004399591':   9,\n",
      "   ...\n",
      "}\n",
      "index of UNK: 32553\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2word = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
    "\n",
    "print('{')\n",
    "for word,_ in zip(word2idx, range(10)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
    "print('   ...\\n}')\n",
    "\n",
    "print('index of UNK: {}'.format(word2idx['UNK']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '마', '뭐야', '늦었네', '후', '버그거렸노', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '기타창에', '넣을', '아이콘', '뭐']\n",
      "[  172 12263 14157  8142 32348 15093   172   417   670   789   860   962\n",
      "  1024  1062  1116     2  6138  7432 19832 14113]\n"
     ]
    }
   ],
   "source": [
    "print(train_text_X[:20])\n",
    "print(text_as_int[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '마' '뭐야' '늦었네' '후' '버그거렸노' '1' '2' '3' '4' '5' '6' '7' '8' '9' '0'\n",
      " '기타창에' '넣을' '아이콘' '뭐' '없노' '\\\\?' '그게' '먼말이노' '기타창' '버튼에']\n",
      "[  172 12263 14157  8142 32348 15093   172   417   670   789   860   962\n",
      "  1024  1062  1116     2  6138  7432 19832 14113 21213  1595  5268 13094\n",
      "  6137 15149]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "examples_per_epoch = len(text_as_int)\n",
    "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sentence_dataset = sentence_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for item in sentence_dataset.take(1):\n",
    "    print(idx2word[item.numpy()])\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '마' '뭐야' '늦었네' '후' '버그거렸노' '1' '2' '3' '4' '5' '6' '7' '8' '9' '0'\n",
      " '기타창에' '넣을' '아이콘' '뭐' '없노' '\\\\?' '그게' '먼말이노' '기타창']\n",
      "[  172 12263 14157  8142 32348 15093   172   417   670   789   860   962\n",
      "  1024  1062  1116     2  6138  7432 19832 14113 21213  1595  5268 13094\n",
      "  6137]\n",
      "버튼에\n",
      "15149\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    return [chunk[:-1], chunk[-1]]\n",
    "\n",
    "train_dataset = sentence_dataset.map(split_input_target)\n",
    "for x, y in train_dataset.take(1):\n",
    "    print(idx2word[x.numpy()])\n",
    "    print(x.numpy())\n",
    "    print(idx2word[y.numpy()])\n",
    "    print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = examples_per_epoch\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 25, 100)           3255400   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 25, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32554)             3287954   \n",
      "=================================================================\n",
      "Total params: 6,704,154\n",
      "Trainable params: 6,704,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(units=100),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 58929 steps\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def testmodel(epoch, logs):\n",
    "    if epoch % 5 != 0 and epoch != 49:\n",
    "        return\n",
    "    test_sentence = train_text[0]\n",
    "    \n",
    "    next_words = 100\n",
    "    for _ in range(next_words):\n",
    "        test_text_X = test_sentence.split(' ')[-seq_length:]\n",
    "        test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
    "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
    "        \n",
    "        output_idx = model.predict_classes(test_text_X)\n",
    "        test_sentence += ' ' + idx2word[output_idx[0]]\n",
    "        \n",
    "    print()\n",
    "    print(test_sentence)\n",
    "    print()\n",
    "    \n",
    "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
    "\n",
    "history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
