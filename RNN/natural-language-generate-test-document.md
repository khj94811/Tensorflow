# Natural Language Generation

- 재미로 친구들과의 카카오톡 단체방 대화 내역을 가져와서 자연어생성 모델에 돌려봤다.

    - LSTM Layer를 이용하였다.
    
    - 결론적으로, 상당히 좋지 못한 성능을 보였다.
    
        - 동일 단어 반복이 많이 일어났다.
        
        - ex). "후 후 후 후 후 후 후 후 후 후 후 .."
        
    - 특히 "후", "?", "아" 등의 한글자 단어가 반복적으로 등장했다.
    
    - 글자수가 2 이상인 단어만 남기고 Training을 하면 달라지지 않을까 하여 Input Data를 살짝 가공해봤다.
      
      - 영문을 알 수 없는 단어가 생성되었다. "인다", "시었다" 등.
    
    - GRU Layer를 이용해볼까?
    
      - 추후 실험 예정
